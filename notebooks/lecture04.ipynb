{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6fe2735",
   "metadata": {},
   "source": [
    "Virtual Acoustics and Immersive Audio Workshop - CCRMA Stanford University  \n",
    "24.07.25 - Orchisama Das, Gloria Dal Santo\n",
    "  \n",
    "### L04: Differentiable Artificial Reverberation\n",
    "\n",
    "In this assignment we will \n",
    "- Create a Differentiable Feedback Delay Network (DiffFDN)\n",
    "- Implement the Loss functions (if you don't have time, you can use those implemented in `flamo.optimize.loss`)\n",
    "- Load the target RIR and construct a Dataset from it \n",
    "- Train the DiffFDN and plot the result and the training/validation loss \n",
    "\n",
    "For this assignment you'll be using [FLAMO library](https://github.com/gdalsanto/flamo).  \n",
    "More specifically, we'll be using classes from \n",
    "- `flamo.processor.dsp` and `flamo.processor.system` to build the DiffFDN\n",
    "- `flamo.optimize.dataset`for the dataset and dataloader \n",
    "- `flamo.optimize.trainer` for the train and validation steps   \n",
    "\n",
    "If you want to use the pretrained parameters, set to True the `is_pretrained` variable in the imports \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdfe389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Imports ---\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd\n",
    "\n",
    "from pathlib import Path\n",
    "from flamo.optimize.loss import edr_loss\n",
    "from flamo.optimize.dataset import Dataset, load_dataset\n",
    "from flamo.optimize.trainer import Trainer\n",
    "from room_acoustics.fdn import DifferentiableFeedbackDelayNetwork\n",
    "from room_acoustics.plot import plot_time_domain, plot_spectrogram\n",
    "from utils import audioread, find_onset\n",
    "from scipy.signal import resample_poly\n",
    "\n",
    "torch.set_default_dtype(torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a39ea7",
   "metadata": {},
   "source": [
    "#### 1. Create a Differentiable Feedback Delay Network\n",
    "\n",
    "\n",
    "- 1.1 Head to `room_acoustics.fdn`, you will find a partially coded class `DifferentiableFeedbackDelayNetwork`. Similarly to the example provided in [flamo](https://github.com/gdalsanto/flamo/blob/c88eb72cb929c6d95e12ce7b103d0de697654e11/examples/e8_fdn.py#L32C5-L32C16), define the different modules that constitute the FDN. \n",
    "- 1.2 In the cell below, create one instance of the DiffFDN.\n",
    "- 1.3 Normalize the energy of the recursive part of the FDN. You can do this by calling the `normalize_late_energy` method.\n",
    "- 1.4 Get the impulse response of the system by calling the `get_time_response` of the class's shell object. \n",
    "- 1.5 Plot it in time domain and its spectrogram. Convolve it with a dry signal to hear how a randomly initialized FDN can sound like. \n",
    "\n",
    "Note: The impulse response is a tensor of shape (batch_size, nfft, output_channels), following the FLAMO convention.\n",
    "When calling `get_time_response`, output_channels = 1 and batch_size = 1. However, to plot it correctly we need to squeeze it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9f2a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Construct the Differentiable FDN ---\n",
    "delay_lengths = torch.tensor([997, 1153, 1327, 1559, 1801, 2099])\n",
    "N = len(delay_lengths)\n",
    "fs = 44100\n",
    "nfft = 2 * fs\n",
    "\n",
    "# create one instance of the DiffFDN\n",
    "diffFDN = DifferentiableFeedbackDelayNetwork(\n",
    "    delay_lengths=delay_lengths,\n",
    "    nfft=nfft,\n",
    "    fs=fs,\n",
    ")\n",
    "\n",
    "# it is good practice to normalize the energy of the FDN, ideally to match the target energy of the RIR\n",
    "diffFDN.normalize_late_energy(target_energy=1.0)\n",
    "\n",
    "# for easiness, let's rename the shell object \n",
    "model = diffFDN.model\n",
    "\n",
    "ir = model.get_time_response()\n",
    "ir = ir.squeeze().numpy()\n",
    "\n",
    "plot_time_domain(ir, fs=fs, title=\"Impulse Response at Init\")\n",
    "plot_spectrogram(ir, fs=fs, title=\"Spectrogram at Init\", clim=[None, None])\n",
    "\n",
    "rec_path = Path('..') / 'data' / 'Drums.wav'\n",
    "rec, rec_fs = audioread(str(rec_path))\n",
    "if rec_fs != fs:\n",
    "    # resample the recording to match the FDN's sampling rate\n",
    "    rec = resample_poly(rec, fs, rec_fs)\n",
    "# convert rec to a torch tensor with appropriate shape and dtype\n",
    "rec_tensor = torch.tensor(rec[:ir.shape[0]], dtype=torch.float32).unsqueeze(0).unsqueeze(-1)\n",
    "# pass through the model and detach the result\n",
    "processed_audio = model(rec_tensor).squeeze().detach().numpy()\n",
    "# Play the processed audio\n",
    "ipd.display(ipd.Audio(processed_audio, rate=fs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb947168",
   "metadata": {},
   "source": [
    "#### 2. Load Target RIR and Create Dataloader\n",
    "\n",
    "- 2.1 Load the target RIR `data/arni_35.wav` and check that its sampling rate matches the model's sampling rate (good practice).\n",
    "- 2.2 Remove the onset from the target RIR to align it properly. Alternatively you can design the onset delay to be equivalent to the target's onset. \n",
    "- 2.3 Match the length of the target RIR to the model's impulse response and normalize its energy. By default the model IR's length is equivalent to`nfft`, number of fft points.\n",
    "- 2.4 Create an impulse input and use it to construct a dataset for training.\n",
    "- 2.5 Generate training and validation dataloaders from the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0ba526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load Target RIR and Create Dataloader ---\n",
    "\n",
    "# Load target RIR\n",
    "#### WRITE YOUR CODE HERE ####\n",
    "# Load the target RIR from a file - call it `target_rir_np`\n",
    "# Check that sampling rate matches the model's sampling rate\n",
    "# Find and remove onset \n",
    "\n",
    "# Match length to model IR and normalize energy\n",
    "ir_len = nfft\n",
    "# We need to make a Torch tensor from the target RIR\n",
    "target_rir = torch.tensor(target_rir_np[:ir_len], dtype=torch.float32).reshape(1, -1, 1)\n",
    "target_rir = target_rir / torch.sqrt(torch.sum(target_rir ** 2))\n",
    "\n",
    "num = 128  # Number of examples in the dataset\n",
    "batch_size = 1  # Batch size for training - here there is no need to increase it (no SGD)\n",
    "# Create impulse input for dataset\n",
    "input_imp = torch.zeros(num, ir_len, 1)\n",
    "input_imp[:, 0, :] = 1\n",
    "\n",
    "# Generate dataset and loaders\n",
    "dataset = Dataset(\n",
    "    input=input_imp,\n",
    "    target=target_rir,\n",
    "    expand=num,\n",
    ")\n",
    "train_loader, valid_loader = load_dataset(dataset, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0e2860",
   "metadata": {},
   "source": [
    "#### 3. Train the Differentiable FDN or Load Pretrained Model\n",
    "\n",
    "- 3.1 Complete the sparsity loss class that will be used to promote temporal density\n",
    "- 3.2 Configure the trainer class. Add the losses using the `register_criterion` method, which will require you to provide a weight `alpha` and whether or not the loss requires access to the model's parameters (this will be set to True when using the sparsity loss, since it need to access to the feedback matrix). To optimize the attenuation filters use the `edr_loss` available in flamo. The EDR loss matches the Energy Decay Relief between the generated and target impulse responses\n",
    "- 3.3 Start the training process with the prepared dataloaders.\n",
    "\n",
    "The training will run for the specified number of epochs, saving checkpoints during the process. After every epoch the trainer will print the training and validation losses. \n",
    "\n",
    "**Note**: Training can take several minutes depending on your hardware (On Mac M1 it takes ~15 mins). The model will learn to match the target RIR's acoustic characteristics. If you have trouble training the model due the limitations of the available computing power, you can load the weights of a pretrained model from `data/checkpoints/.`. Set `is_pretrained`to True in that case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee8801f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load Pretrained Model or Start Training ---\n",
    "\n",
    "# Implement the losses \n",
    "class sparsity_loss(nn.Module):\n",
    "    r\"\"\"\n",
    "    Calculates the sparsity loss for a given model.\n",
    "\n",
    "    The sparsity loss is calculated based on the feedback loop of the FDN model's core.\n",
    "    It measures the sparsity of the feedback matrix A of size (N, N).\n",
    "    Note: for the loss to be compatible with the class :class:`flamo.optimize.trainer.Trainer`, it requires :attr:`y_pred` and :attr:`y_target` as arguments even if these are not being considered.\n",
    "    If the feedback matrix has a third dimension C, A.size = (C, N, N), the loss is calculated as the mean of the contribution of each (N,N) matrix.\n",
    "\n",
    "    .. math::\n",
    "\n",
    "        \\mathcal{L} = \\frac{\\sum_{i,j} |A_{i,j}| - N\\sqrt{N}}{N(1 - \\sqrt{N})}\n",
    "\n",
    "    For more details, refer to the paper `Optimizing Tiny Colorless Feedback Delay Networks <https://arxiv.org/abs/2402.11216>`_ by Dal Santo, G. et al.\n",
    "\n",
    "    **Arguments**:\n",
    "        - **y_pred** (torch.Tensor): The predicted output.\n",
    "        - **y_target** (torch.Tensor): The target output.\n",
    "        - **model** (nn.Module): The model containing the core with the feedback loop.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The calculated sparsity loss.\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, y_pred: torch.Tensor, y_target: torch.Tensor, model: nn.Module):\n",
    "        # This is how we access the feedback loop's mixing matrix\n",
    "        core = model.get_core()\n",
    "        A = core.branchA.feedback_loop.feedback.mixing_matrix.map(\n",
    "            core.branchA.feedback_loop.feedback.mixing_matrix.param\n",
    "        )\n",
    "        N = A.shape[-1]\n",
    "        #### WRITE YOUR CODE HERE ####\n",
    "        # Calculate the sparsity loss\n",
    "        return loss\n",
    "    \n",
    "is_pretrained = False # Set to True if you want to load a pretrained model instead of training from scratch\n",
    "if not is_pretrained:\n",
    "    print(\"Starting training from scratch.\")\n",
    "    # Initialize training process\n",
    "    max_epochs = 20\n",
    "    alpha = [2.0, 1.0]\n",
    "    \n",
    "    #### WRITE YOUR CODE HERE ####\n",
    "    # Create the trainer class \n",
    "    # Register the losses using `trainer.register_criterion` you have to do it twice, once for the sparsity loss and once for the EDR loss\n",
    "    # Start training\n",
    "    trainer.train(train_dataset=train_loader, valid_dataset=valid_loader)\n",
    "\n",
    "if is_pretrained:\n",
    "    # Find the latest checkpoint\n",
    "    checkpoints = sorted(\n",
    "        (Path(\"..\") / \"output\" / \"checkpoints\").glob(\"*.pt\"),\n",
    "        key=lambda x: x.stat().st_mtime,\n",
    "    )\n",
    "    if checkpoints:\n",
    "        latest_checkpoint = checkpoints[-1]\n",
    "        print(f\"Loading model from {latest_checkpoint}\")\n",
    "        model.load_state_dict(\n",
    "            torch.load(latest_checkpoint, map_location=torch.device(\"cpu\"))\n",
    "        )\n",
    "        model.eval()\n",
    "        print(\"Model loaded successfully.\")\n",
    "    else:\n",
    "        print(\"No checkpoints found.\")\n",
    "        is_pretrained = False\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4183906",
   "metadata": {},
   "source": [
    "#### 4. Analyze Training Results and Compare with Target\n",
    "\n",
    "- 4.1 If training was performed, visualize the training and validation losses over epochs to understand how well the model converged, and the relative \n",
    "- 4.2 Analyze the learned impulse response against the target by plotting them in time domain and their spectrogram. You could also run EDC analysis. \n",
    "- 4.3 Process an anechoic signal with the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2913d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not is_pretrained:\n",
    "    # Create a plot for training loss\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for i, key in enumerate(trainer.train_loss_log.keys()):\n",
    "        n_steps = len(trainer.train_loss_log[key])\n",
    "        plt.plot(np.array(trainer.train_loss_log[key][:: n_steps // max_epochs]) * alpha[i], label=f\"Train {key}\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Create a plot for validation loss\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for i, key in enumerate(trainer.valid_loss_log.keys()):\n",
    "        n_steps = len(trainer.valid_loss_log[key])\n",
    "        plt.plot(\n",
    "            np.array(trainer.valid_loss_log[key][:: n_steps // max_epochs]) * alpha[i],\n",
    "            label=f\"Valid {key}\",\n",
    "        )\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Validation Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Analyze the results\n",
    "plot_time_domain(\n",
    "    model.get_time_response().squeeze().numpy(),\n",
    "    fs=fs,\n",
    "    title=\"Learned Impulse Response at optim\",\n",
    ")\n",
    "\n",
    "ir_out =  model.get_time_response().squeeze().numpy()\n",
    "plot_spectrogram(\n",
    "    ir_out / np.sqrt(np.sum(ir_out ** 2)),\n",
    "    fs=fs,\n",
    "    title=\"Learned Impulse Response at optim\",\n",
    "    clim=[-170, -50],\n",
    ")\n",
    "\n",
    "plot_spectrogram(\n",
    "    target_rir.squeeze().numpy(),\n",
    "    fs=fs,\n",
    "    title=\"Target Impulse Response\",\n",
    "    clim=[-170, -50],\n",
    ")\n",
    "\n",
    "\n",
    "# pass through the model and detach the result\n",
    "processed_audio = model(rec_tensor).squeeze().detach().numpy()\n",
    "# Play the processed audio\n",
    "ipd.display(ipd.Audio(processed_audio, rate=fs))\n",
    "\n",
    "ir = model.get_time_response().squeeze().numpy()\n",
    "ipd.display(ipd.Audio(ir, rate=fs))\n",
    "print(f\"Convolution with FDN IR\")\n",
    "ipd.display(ipd.Audio(np.convolve(ir, rec, 'full'), rate=fs))\n",
    "print(f\"Convolution with Target RIR\")\n",
    "ipd.display(ipd.Audio(np.convolve(target_rir.squeeze().numpy(), rec, 'full'), rate=fs))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
